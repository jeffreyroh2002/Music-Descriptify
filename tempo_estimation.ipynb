{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyroh2002/Music-Descriptify/blob/main/tempo_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1gvMqcy7wkA"
      },
      "outputs": [],
      "source": [
        "!pip install ghp-import\n",
        "!pip install mirdata>=0.3.0\n",
        "!pip install librosa\n",
        "!pip install cython\n",
        "!pip install madmom\n",
        "!pip install mir_eval\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l48UvAg7O4JK",
        "outputId": "249f879d-fb8c-4c5f-f359-43a73f05772f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LASENqkgwwoj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import (\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Input,\n",
        "    Conv1D,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Reshape,\n",
        "    Dropout,\n",
        "    SpatialDropout1D,\n",
        "    GaussianNoise,\n",
        "    GlobalAveragePooling1D,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk8LL1t86_MM"
      },
      "source": [
        "****Tempo, Beat and Downbeat Estimation****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbLxdZIiJU6s"
      },
      "source": [
        "-Downloading gtzan dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9D94lPs7Eaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec62072-30b5-4748-ced1-472c56890560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cwb2vAKryAYqkP2D86bpO6mbVlQmVhjN\n",
            "To: /root/mir_datasets/gtzan_genre/gtzan_genre/genres.tar.gz\n",
            "100% 1.23G/1.23G [00:14<00:00, 84.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "632kB [00:00, 1.41MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import mirdata\n",
        "\n",
        "# gtzan = mirdata.initialize('gtzan_genre', version = 'mini')\n",
        "# gtzan.download()\n",
        "# gtzan.validate()\n",
        "# len(gtzan.track_ids)\n",
        "\n",
        "# obtain copy of GTZAN data (use a mirror URL for faster access)\n",
        "!pip install gdown\n",
        "!mkdir -p /root/mir_datasets/gtzan_genre/gtzan_genre/\n",
        "!gdown --id 1cwb2vAKryAYqkP2D86bpO6mbVlQmVhjN --output /root/mir_datasets/gtzan_genre/gtzan_genre/genres.tar.gz\n",
        "# use the following line to initialise the dataset (i.e. the full version without 'mini')\n",
        "gtzan = mirdata.initialize('gtzan_genre')\n",
        "gtzan.download()\n",
        "len(gtzan.track_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqi3dE2XJarE"
      },
      "source": [
        "-Define dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpUCIwg5JcpL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tracks = gtzan.load_tracks()\n",
        "train_files, test_files = train_test_split(list(tracks.keys()), test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSq6rjv6MtLw"
      },
      "source": [
        "-Audio pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7WY2f03MvcQ"
      },
      "outputs": [],
      "source": [
        "from madmom.processors import ParallelProcessor, SequentialProcessor\n",
        "from madmom.audio.signal import SignalProcessor, FramedSignalProcessor\n",
        "from madmom.audio.stft import ShortTimeFourierTransformProcessor\n",
        "from madmom.audio.spectrogram import FilteredSpectrogramProcessor, LogarithmicSpectrogramProcessor\n",
        "import numpy as np\n",
        "\n",
        "FPS = 100\n",
        "FFT_SIZE = 2048\n",
        "NUM_BANDS = 12\n",
        "\n",
        "class PreProcessor(SequentialProcessor):\n",
        "  def __init__(self, frame_size = FFT_SIZE, num_bands = NUM_BANDS, log = np.log, add = 1e-6, fps = FPS):\n",
        "    #The signalProcessor class is a basic signal processor\n",
        "    #it works like a librosa.load function\n",
        "    sig = SignalProcessor(num_channels = 1, sample_rate = 44100)\n",
        "    frames = FramedSignalProcessor(frame_size = frame_size, fps = fps)\n",
        "    stft = ShortTimeFourierTransformProcessor()\n",
        "    filt = FilteredSpectrogramProcessor(num_bands = num_bands)\n",
        "    spec = LogarithmicSpectrogramProcessor(log = log, add = add)\n",
        "    super(PreProcessor, self).__init__((sig, frames, stft, filt, spec, np.array))\n",
        "    self.fps = fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO-oDh5hOy5x"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, i, activation, num_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    # name of the layer\n",
        "    name = name + '_dilation_%d' % i\n",
        "    # 1x1 conv. of input (so it can be added as residual)\n",
        "    res_x = Conv1D(num_filters, 1, padding='same', name=name + '_1x1_conv_residual')(x)\n",
        "    # two dilated convolutions, with dilation rates of i and 2i\n",
        "    conv_1 = Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=i,\n",
        "        padding=padding,\n",
        "        name=name + '_dilated_conv_1',\n",
        "    )(x)\n",
        "    conv_2 = Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=i * 2,\n",
        "        padding=padding,\n",
        "        name=name + '_dilated_conv_2',\n",
        "    )(x)\n",
        "    # concatenate the output of the two dilations\n",
        "    concat = keras.layers.concatenate([conv_1, conv_2], name=name + '_concat')\n",
        "    # apply activation function\n",
        "    x = Activation(activation, name=name + '_activation')(concat)\n",
        "    # apply spatial dropout\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout_%f' % dropout_rate)(x)\n",
        "    # 1x1 conv. to obtain a representation with the same size as the residual\n",
        "    x = Conv1D(num_filters, 1, padding='same', name=name + '_1x1_conv')(x)\n",
        "    # add the residual to the processed data and also return it as skip connection\n",
        "    return keras.layers.add([res_x, x], name=name + '_merge_residual'), x\n",
        "\n",
        "\n",
        "class TCN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters=20,\n",
        "        kernel_size=5,\n",
        "        dilations=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
        "        activation='elu',\n",
        "        padding='same',\n",
        "        dropout_rate=0.15,\n",
        "        name='tcn',\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_filters = num_filters\n",
        "        self.padding = padding\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        # gather skip connections, each having a different context\n",
        "        skip_connections = []\n",
        "        # build the TCN models\n",
        "        for i, num_filters in zip(self.dilations, self.num_filters):\n",
        "            # feed the output of the previous layer into the next layer\n",
        "            # increase dilation rate for each consecutive layer\n",
        "            x, skip_out = residual_block(\n",
        "                x, i, self.activation, num_filters, self.kernel_size, self.padding, self.dropout_rate, name=self.name\n",
        "            )\n",
        "            # collect skip connection\n",
        "            skip_connections.append(skip_out)\n",
        "        # activate the output of the TCN stack\n",
        "        x = Activation(self.activation, name=self.name + '_activation')(x)\n",
        "        # merge the skip connections by simply adding them\n",
        "        skip = keras.layers.add(skip_connections, name=self.name + '_merge_skip_connections')\n",
        "        return x, skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDD7ZM1Bwf2t"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape, num_filters=20, num_dilations=11, kernel_size=5, activation='elu', dropout_rate=0.15):\n",
        "    # input layer\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # stack of 3 conv layers, each conv, activation, max. pooling & dropout\n",
        "    conv_1 = Conv2D(num_filters, (3, 3), padding='valid', name='conv_1_conv')(input_layer)\n",
        "    conv_1 = Activation(activation, name='conv_1_activation')(conv_1)\n",
        "    conv_1 = MaxPooling2D((1, 3), name='conv_1_max_pooling')(conv_1)\n",
        "    conv_1 = Dropout(dropout_rate, name='conv_1_dropout')(conv_1)\n",
        "\n",
        "    conv_2 = Conv2D(num_filters, (1, 10), padding='valid', name='conv_2_conv')(conv_1)\n",
        "    conv_2 = Activation(activation, name='conv_2_activation')(conv_2)\n",
        "    conv_2 = MaxPooling2D((1, 3), name='conv_2_max_pooling')(conv_2)\n",
        "    conv_2 = Dropout(dropout_rate, name='conv_2_dropout')(conv_2)\n",
        "\n",
        "    conv_3 = Conv2D(num_filters, (3, 3), padding='valid', name='conv_3_conv')(conv_2)\n",
        "    conv_3 = Activation(activation, name='conv_3_activation')(conv_3)\n",
        "    conv_3 = MaxPooling2D((1, 3), name='conv_3_max_pooling')(conv_3)\n",
        "    conv_3 = Dropout(dropout_rate, name='conv_3_dropout')(conv_3)\n",
        "\n",
        "    # reshape layer to reduce dimensions\n",
        "    x = Reshape((-1, num_filters), name='tcn_input_reshape')(conv_3)\n",
        "\n",
        "    # TCN layers\n",
        "    dilations = [2 ** i for i in range(num_dilations)]\n",
        "    tcn, skip = TCN(\n",
        "        num_filters=[num_filters] * len(dilations),\n",
        "        kernel_size=kernel_size,\n",
        "        dilations=dilations,\n",
        "        activation=activation,\n",
        "        padding='same',\n",
        "        dropout_rate=dropout_rate,\n",
        "    )(x)\n",
        "\n",
        "    # output layers; beats & downbeats use TCN output, tempo the skip connections\n",
        "\n",
        "    tempo = Dropout(dropout_rate, name='tempo_dropout')(skip)\n",
        "    tempo = GlobalAveragePooling1D(name='tempo_global_average_pooling')(tempo)\n",
        "    tempo = GaussianNoise(dropout_rate, name='tempo_noise')(tempo)\n",
        "    tempo = Dense(250, name='tempo_dense')(tempo)\n",
        "    tempo = Activation('softmax', name='tempo')(tempo)\n",
        "\n",
        "    # instantiate a Model and return it\n",
        "    return Model(input_layer, outputs=tempo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-IOEjyrzzSf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from keras.utils import Sequence\n",
        "import madmom\n",
        "\n",
        "MASK_VALUE = -1\n",
        "\n",
        "class DataSequence(Sequence):\n",
        "  def __init__(self, tracks, pre_processor, num_tempo_bins = 250, pad_frames = None):\n",
        "    self.x = {}\n",
        "    self.tempo = {}\n",
        "    self.pad_frames = pad_frames\n",
        "    self.ids = []\n",
        "\n",
        "    for i, key in enumerate(tracks):\n",
        "      sys.stderr.write(f'\\rprocessing track {i + 1}/{len(tracks)}: {key + \" \" * 20}')\n",
        "      sys.stderr.flush()\n",
        "      t = tracks[key]\n",
        "      try:\n",
        "        beats = t.beats.times\n",
        "        s = madmom.audio.Signal(*t.audio)\n",
        "        tempo = t.tempo\n",
        "        tempo = keras.utils.to_categorical(int(np.round(tempo)), num_classes=num_tempo_bins, dtype='float32')\n",
        "        tempo = tf.constant(tempo)\n",
        "        tempo = tf.expand_dims(tempo, axis = 0)\n",
        "        self.tempo[key] = tempo\n",
        "        x = pre_processor(s)\n",
        "        x = tf.constant(x)\n",
        "        x = tf.expand_dims(x, axis = 0)\n",
        "        x = tf.expand_dims(x, axis = -1)\n",
        "        self.x[key] = x\n",
        "        self.ids.append(key)\n",
        "      except AttributeError:\n",
        "        print(f'\\r{key} has no tempo information, skipping\\n')\n",
        "        continue\n",
        "      except IndexError:\n",
        "        continue\n",
        "      assert len(self.x) == len(self.tempo) == len(self.ids)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if isinstance(idx, int):\n",
        "      idx = self.ids[idx]\n",
        "      x = self.x[idx]\n",
        "      y = self.tempo[idx]\n",
        "      return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm6Vt1pazt5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f94a847-c428-4882-9d4b-35f73415d548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing track 633/800: reggae.00088                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rreggae.00086 has no tempo information, skipping\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing track 200/200: metal.00098                    "
          ]
        }
      ],
      "source": [
        "pad_frames = 2\n",
        "pre_processor = PreProcessor()\n",
        "\n",
        "train = DataSequence(\n",
        "    tracks={k: v for k, v in tracks.items() if k in train_files}, pre_processor=pre_processor, pad_frames=pad_frames\n",
        ")\n",
        "test = DataSequence(\n",
        "    tracks={k: v for k, v in tracks.items() if k in test_files}, pre_processor=pre_processor, pad_frames=pad_frames\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO7ofeykgzWX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "680d3191-f5e3-47eb-e86b-6005afbc8bad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class '__main__.DataSequence'>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5485fc0cb478>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[0munsplitable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_can_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsplitable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1777\u001b[0m             \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0;34m\"arrays, found following types in the input: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsplitable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class '__main__.DataSequence'>]"
          ]
        }
      ],
      "source": [
        "model = create_model(input_shape = train[0][0].shape[-3:])\n",
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.0001), loss = keras.losses.BinaryCrossentropy(), metrics = [\"accuracy\",])\n",
        "model.fit(train, epochs = 100, shuffle = True, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"tempo.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpFm1Qzgg2PR",
        "outputId": "f043710e-8b1c-4d5e-e9f9-6ec8e507d640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aTzAEvYp4IW",
        "outputId": "a4575d32-078e-424a-df6d-8fe4da661c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - 7s 14ms/step - loss: 0.0245 - accuracy: 0.3850\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.024480147287249565, 0.38499999046325684]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "id": "FjeLEsuosMEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = PreProcessor()\n",
        "for i in range(20):\n",
        "  id = test.ids[i]\n",
        "  tempo = tracks[id].tempo\n",
        "  prediction = model.predict(test[i][0])\n",
        "  prediction = tf.argmax(prediction, axis = -1)\n",
        "  print(prediction)\n",
        "  print(tempo)\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFLI362krYW4",
        "outputId": "d44eede8-8826-4eef-82dd-5e20811509bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "tf.Tensor([104], shape=(1,), dtype=int64)\n",
            "103.44\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "tf.Tensor([63], shape=(1,), dtype=int64)\n",
            "92.66\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "tf.Tensor([132], shape=(1,), dtype=int64)\n",
            "131.99\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "tf.Tensor([61], shape=(1,), dtype=int64)\n",
            "119.58\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "tf.Tensor([52], shape=(1,), dtype=int64)\n",
            "65.67\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "tf.Tensor([69], shape=(1,), dtype=int64)\n",
            "81.81\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "tf.Tensor([97], shape=(1,), dtype=int64)\n",
            "87.37\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "tf.Tensor([52], shape=(1,), dtype=int64)\n",
            "55.43\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "tf.Tensor([117], shape=(1,), dtype=int64)\n",
            "115.95\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "tf.Tensor([120], shape=(1,), dtype=int64)\n",
            "120.51\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "tf.Tensor([95], shape=(1,), dtype=int64)\n",
            "93.59\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "tf.Tensor([120], shape=(1,), dtype=int64)\n",
            "127.52\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "tf.Tensor([64], shape=(1,), dtype=int64)\n",
            "124.09\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "tf.Tensor([162], shape=(1,), dtype=int64)\n",
            "159.76\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "tf.Tensor([120], shape=(1,), dtype=int64)\n",
            "117.3\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "tf.Tensor([172], shape=(1,), dtype=int64)\n",
            "171.9\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "tf.Tensor([52], shape=(1,), dtype=int64)\n",
            "65.46\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "tf.Tensor([101], shape=(1,), dtype=int64)\n",
            "100.45\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "tf.Tensor([104], shape=(1,), dtype=int64)\n",
            "104.39\n",
            "\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "tf.Tensor([139], shape=(1,), dtype=int64)\n",
            "133.12\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "id": "1WFZ5vcxsvut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54af5c5b-c868-479a-d1e0-f36dbae48cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config -- global user.email \"dongdong1653@gmail.com\"\n",
        "!git config -- global user.name \"Joey-tpop\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJIeNYfI87QY",
        "outputId": "8a5153c6-7c5d-4f98-cc74-a68a94a5a936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: key does not contain a section: global\n",
            "error: key does not contain a section: global\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4ZbyALr9LVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS527lHQJQs1NOU0fVMQrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}