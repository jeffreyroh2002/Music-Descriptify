{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyroh2002/Music-Descriptify/blob/main/instr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download your dataset using wget keyword\n",
        "!wget https://zenodo.org/records/1432913/files/openmic-2018-v1.0.0.tgz?download=1\n",
        "#unzip your dataset\n",
        "!tar -xzvf openmic-2018-v1.0.0.tgz?download=1\n",
        "#I recommend you to delete the tgz file for your disk storage\n",
        "!rm openmic-2018-v1.0.0.tgz?download=1"
      ],
      "metadata": {
        "id": "CbA_GkoaOgmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there are some requirements needed to run the codes below\n",
        "!pip install madmom==0.16.1"
      ],
      "metadata": {
        "id": "YBj9KORSxpom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the npz file**"
      ],
      "metadata": {
        "id": "IJqCktBUwWQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#set some paths and load the npz file\n",
        "AUDIO_PATH = \"openmic-2018/audio/\"\n",
        "NPZ_PATH = \"openmic-2018/openmic-2018.npz\"\n",
        "TRAIN_SPLIT_PATH = \"openmic-2018/partitions/split01_train.csv\"\n",
        "TEST_SPLIT_PATH = \"openmic-2018/partitions/split01_test.csv\"\n",
        "\n",
        "try:\n",
        "  OPENMIC = np.load(NPZ_PATH, allow_pickle = True)\n",
        "except FileNotFoundError:\n",
        "  print(\"[Error] Cannot find the npz file! Check if you downloaded your dataset properly\")\n",
        "\n",
        "Y_true, Y_mask, sample_key = OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']"
      ],
      "metadata": {
        "id": "oyOhH7PfoVXU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the split files**"
      ],
      "metadata": {
        "id": "AOB5ErlZwaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "try:\n",
        "  split_train = pd.read_csv(TRAIN_SPLIT_PATH, header = None)\n",
        "  split_test = pd.read_csv(TEST_SPLIT_PATH, header = None)\n",
        "except FileNotFoundError:\n",
        "  print(\"[Error] Cannot find the split filepath! Check if you downloaded your dataset properly\")\n",
        "\n",
        "#these variables contain keys of each audio file\n",
        "train_set = split_train[0].values         #total 14915 files\n",
        "test_set = split_test[0].values           #total 5085 files\n",
        "print(len(train_set), len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s9mdpfKqe_K",
        "outputId": "6e662406-087a-44a6-d95c-64a93a06fb89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14915 5085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "audio_filenames = glob.glob(AUDIO_PATH + \"*/*.ogg\")\n",
        "assert len(audio_filenames) >= 1, \"No audio files are found! Check if you downloaded your dataset properly\"\n",
        "\n",
        "for i, filename in enumerate(audio_filenames):\n",
        "  key = filename.split('/')[-1]\n",
        "  os.rename(filename, AUDIO_PATH + key)\n",
        "  if i % 1000 == 0:\n",
        "    print(f\"{i}th file renaming completed!\")\n",
        "\n",
        "empty_dirs = glob.glob(AUDIO_PATH + \"???\")\n",
        "print(\"removing empty directories...\")\n",
        "for dir in empty_dirs:\n",
        "  os.rmdir(dir)\n",
        "\n",
        "audio_filenames = glob.glob(AUDIO_PATH + \"*.ogg\")"
      ],
      "metadata": {
        "id": "K3CBG4xtwfdt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9407f2a0-417c-423a-d585-52c553342a97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0th file renaming completed!\n",
            "1000th file renaming completed!\n",
            "2000th file renaming completed!\n",
            "3000th file renaming completed!\n",
            "4000th file renaming completed!\n",
            "5000th file renaming completed!\n",
            "6000th file renaming completed!\n",
            "7000th file renaming completed!\n",
            "8000th file renaming completed!\n",
            "9000th file renaming completed!\n",
            "10000th file renaming completed!\n",
            "11000th file renaming completed!\n",
            "12000th file renaming completed!\n",
            "13000th file renaming completed!\n",
            "14000th file renaming completed!\n",
            "15000th file renaming completed!\n",
            "16000th file renaming completed!\n",
            "17000th file renaming completed!\n",
            "18000th file renaming completed!\n",
            "19000th file renaming completed!\n",
            "removing empty directories...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the processor\n",
        "# try:\n",
        "from madmom.processors import ParallelProcessor, SequentialProcessor\n",
        "from madmom.audio.signal import SignalProcessor, FramedSignalProcessor\n",
        "from madmom.audio.stft import ShortTimeFourierTransformProcessor\n",
        "from madmom.audio.spectrogram import FilteredSpectrogramProcessor, LogarithmicSpectrogramProcessor\n",
        "\n",
        "# except ImportError:\n",
        "#   raise ImportError\n",
        "#   print(\"Go to the file [/usr/local/lib/{python_version}/dist-packages/madmom/processors.py] and change collections -> collections.abc in line 23\")\n",
        "\n",
        "FPS = 100\n",
        "FFT_SIZE = 2048\n",
        "NUM_BANDS = 12\n",
        "\n",
        "class PreProcessor(SequentialProcessor):\n",
        "  def __init__(self, frame_size = FFT_SIZE, num_bands = NUM_BANDS, log = np.log, add = 1e-6, fps = FPS):\n",
        "    #The signalProcessor class is a basic signal processor\n",
        "    #it works like a librosa.load function\n",
        "    sig = SignalProcessor(num_channels = 1, sample_rate = 44100)\n",
        "    frames = FramedSignalProcessor(frame_size = frame_size, fps = fps)\n",
        "    stft = ShortTimeFourierTransformProcessor()\n",
        "    filt = FilteredSpectrogramProcessor(num_bands = num_bands)\n",
        "    spec = LogarithmicSpectrogramProcessor(log = log, add = add)\n",
        "    super(PreProcessor, self).__init__((sig, frames, stft, filt, spec, np.array))\n",
        "    self.fps = fps"
      ],
      "metadata": {
        "id": "R7ZxprmMzGOC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_train = []\n",
        "idx_test = []\n",
        "for i, key in enumerate(sample_key):\n",
        "  if key in train_set:\n",
        "    idx_train.append(i)\n",
        "  else:\n",
        "    idx_test.append(i)\n",
        "\n",
        "idx_valid = idx_test[:int(len(idx_test) * 0.1)]\n",
        "idx_test = idx_test[int(len(idx_test) * 0.1):]\n",
        "\n",
        "print(len(idx_train), len(idx_valid), len(idx_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVm1h2urD5jF",
        "outputId": "120f1b4a-3832-4bf6-a253-fbacbe9d8d33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14915 508 4577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def train_generator():\n",
        "  preprocessor = PreProcessor()\n",
        "  label_num = 20\n",
        "  for index in idx_train:\n",
        "    key = sample_key[index]\n",
        "    audio_filename = AUDIO_PATH + key + \".ogg\"\n",
        "    s = preprocessor(audio_filename)\n",
        "    y_true = Y_true[index]\n",
        "    y_mask = Y_mask[index]\n",
        "    y = np.where(y_mask, y_true, -1)\n",
        "\n",
        "    # (tf.float64, tf.float64), ((1000, 81, 1,), (20,))\n",
        "    x = tf.constant(s[:1000][:], dtype = tf.float64)\n",
        "    x = tf.expand_dims(x, axis = -1)\n",
        "    y = tf.constant(y, dtype = tf.float64)\n",
        "\n",
        "    yield (x, y)\n",
        "\n",
        "def valid_generator():\n",
        "  preprocessor = PreProcessor()\n",
        "  label_num = 20\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i, index in enumerate(idx_valid):\n",
        "    if i % 100 == 0:\n",
        "      print(f\"{i}th  file completed\")\n",
        "    key = sample_key[index]\n",
        "    audio_filename = AUDIO_PATH + key + \".ogg\"\n",
        "    s = preprocessor(audio_filename)\n",
        "    y_true = Y_true[index]\n",
        "    y_mask = Y_mask[index]\n",
        "    y = np.where(y_mask, y_true, -1)\n",
        "\n",
        "    # (tf.float64, tf.float64), ((1000, 81, 1,), (20,))\n",
        "    x = tf.constant(s[:1000][:], dtype = tf.float64)\n",
        "    x = tf.expand_dims(x, axis = -1)\n",
        "    x = tf.expand_dims(x, axis = 0)\n",
        "    y = tf.constant(y, dtype = tf.float64)\n",
        "    y = tf.expand_dims(y, axis = 0)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = tf.stack(X)\n",
        "  Y = tf.stack(Y)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "aCyf18orCS-1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, i, activation, num_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    # name of the layer\n",
        "    name = name + '_dilation_%d' % i\n",
        "    # 1x1 conv. of input (so it can be added as residual)\n",
        "    res_x = keras.layers.Conv1D(num_filters, 1, padding='same')(x)\n",
        "    # two dilated convolutions, with dilation rates of i and 2i\n",
        "    conv_1 = keras.layers.Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=i,\n",
        "        padding=padding,\n",
        "    )(x)\n",
        "    conv_2 = keras.layers.Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=i * 2,\n",
        "        padding=padding,\n",
        "    )(x)\n",
        "    concat = keras.layers.concatenate([conv_1, conv_2])\n",
        "    x = keras.layers.Activation(activation)(concat)\n",
        "    x = keras.layers.SpatialDropout1D(dropout_rate)(x)\n",
        "    x = keras.layers.Conv1D(num_filters, 1, padding='same')(x)\n",
        "    return keras.layers.add([res_x, x]), x\n",
        "\n",
        "\n",
        "class TCN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters=20,\n",
        "        kernel_size=5,\n",
        "        dilations=[1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
        "        activation='elu',\n",
        "        padding='same',\n",
        "        dropout_rate=0.15,\n",
        "        name='tcn',\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_filters = num_filters\n",
        "        self.padding = padding\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        # gather skip connections, each having a different context\n",
        "        skip_connections = []\n",
        "        # build the TCN models\n",
        "        for i in self.dilations:\n",
        "            # feed the output of the previous layer into the next layer\n",
        "            # increase dilation rate for each consecutive layer\n",
        "            x, skip_out = residual_block(\n",
        "                x, i, self.activation, 20, self.kernel_size, self.padding, self.dropout_rate, name=self.name\n",
        "            )\n",
        "            # collect skip connection\n",
        "            skip_connections.append(skip_out)\n",
        "        # activate the output of the TCN stack\n",
        "        x = keras.layers.Activation(self.activation, name=self.name + '_activation')(x)\n",
        "        # merge the skip connections by simply adding them\n",
        "        skip = keras.layers.add(skip_connections, name=self.name + '_merge_skip_connections')\n",
        "        return x, skip\n",
        "\n",
        "def create_model(dropout_rate = 0.15, num_filters = 20):\n",
        "  input = keras.layers.Input(shape = (1000, 81, 1))\n",
        "  conv_1 = keras.layers.Conv2D(filters = num_filters, kernel_size = (3, 3), padding = \"valid\",)(input)\n",
        "  conv_1 = keras.layers.Activation(activation = \"elu\")(conv_1)\n",
        "  conv_1 = keras.layers.MaxPooling2D((1, 3))(conv_1)\n",
        "  conv_1 = keras.layers.Dropout(rate = dropout_rate)(conv_1)\n",
        "\n",
        "  conv_2 = keras.layers.Conv2D(filters = num_filters, kernel_size = (1, 10), padding = \"valid\",)(conv_1)\n",
        "  conv_2 = keras.layers.Activation(activation = \"elu\")(conv_2)\n",
        "  conv_2 = keras.layers.MaxPooling2D((1, 3))(conv_2)\n",
        "  conv_2 = keras.layers.Dropout(rate = dropout_rate)(conv_2)\n",
        "\n",
        "  conv_3 = keras.layers.Conv2D(filters = num_filters, kernel_size = (3, 3), padding = \"valid\",)(conv_2)\n",
        "  conv_3 = keras.layers.Activation(activation = \"elu\")(conv_3)\n",
        "  conv_3 = keras.layers.MaxPooling2D((1, 3))(conv_3)\n",
        "  conv_3 = keras.layers.Dropout(rate = dropout_rate)(conv_3)\n",
        "\n",
        "  x = keras.layers.Reshape((-1, num_filters))(conv_3)\n",
        "  _, skip = TCN()(x)\n",
        "\n",
        "  instr = keras.layers.Dropout(dropout_rate, name='output_dropout')(skip)\n",
        "  instr = keras.layers.GlobalAveragePooling1D(name='output_global_average_pooling')(instr)\n",
        "  instr = keras.layers.GaussianNoise(dropout_rate, name='output_noise')(instr)\n",
        "  instr = keras.layers.Dense(20, name='output_dense')(instr)\n",
        "  instr = keras.layers.Activation('softmax', name='output')(instr)\n",
        "\n",
        "  return keras.Model(input, instr)\n",
        "\n"
      ],
      "metadata": {
        "id": "xOlMldCAeBHa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_BCE(y_true, y_pred):\n",
        "  mask = tf.math.not_equal(y_true, -1)\n",
        "  y_true_masked = tf.boolean_mask(y_true, mask)\n",
        "  y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
        "  y_pred_masked = tf.cast(y_pred_masked, dtype = tf.float64)\n",
        "  y_true_masked = tf.cast(y_true_masked > 0.5, tf.float64)\n",
        "  loss = -tf.reduce_mean(y_true_masked * tf.math.log(y_pred_masked + 1e-6) + (1 - y_true_masked) * tf.math.log(1 - y_pred_masked + 1e-6))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "7QVIMe1-tZcr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class custom_ACC(tf.keras.metrics.Metric):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.correct = self.add_weight(initializer = \"zeros\", dtype = tf.int32, name = \"correct\")\n",
        "    self.total = self.add_weight(initializer = \"zeros\", dtype = tf.int32, name = \"total\")\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight = None):\n",
        "    mask = tf.math.not_equal(y_true, -1)\n",
        "\n",
        "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
        "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
        "    y_true_masked = tf.cast(y_true_masked > 0.5, tf.bool)\n",
        "    y_pred_masked = tf.cast(y_pred_masked > 0.5, tf.bool)\n",
        "\n",
        "    correct = tf.math.equal(y_true_masked, y_pred_masked)\n",
        "    correct = tf.cast(correct, dtype = tf.int32)\n",
        "\n",
        "    self.correct.assign_add(tf.reduce_sum(correct))\n",
        "    self.total.assign_add(tf.size(correct))\n",
        "\n",
        "  def result(self):\n",
        "    return self.correct / self.total\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.correct.assign(0)\n",
        "    self.total.assign(0)"
      ],
      "metadata": {
        "id": "Pa2Uzp4XxdPr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(train_generator, (tf.float64, tf.float64), ((1000, 81, 1,), (20,)))\n",
        "valid_dataset = valid_generator()\n",
        "train_dataset = train_dataset.batch(1)"
      ],
      "metadata": {
        "id": "n_E-j3yts0zE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              loss = custom_BCE,\n",
        "              metrics = [custom_ACC()])\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"instr_model.h5\", save_best_only = True,\n",
        "                                                   monitor = custom_ACC(),\n",
        "                                                   mode = \"min\", verbose = 1)\n",
        "earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience = 10, min_delta = 1e-2)\n",
        "model.fit(train_dataset, validation_data = valid_dataset, epochs = 150, callbacks = [checkpoint_cb, earlystopping_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tSmQkMoL-hI",
        "outputId": "d7e4b3c4-549c-4bcb-d8d8-f9063fb234d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "  14915/Unknown - 3584s 239ms/step - loss: 1.3472 - custom_acc_26: 0.5888"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with custom_ACC(name=custom_acc_27,dtype=float32) available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,custom_acc_26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14915/14915 [==============================] - 3584s 239ms/step - loss: 1.3472 - custom_acc_26: 0.5888\n",
            "Epoch 2/150\n",
            " 2713/14915 [====>.........................] - ETA: 49:32 - loss: 1.2390 - custom_acc_26: 0.5765"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6PQk2hVa0Fk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6fV3jg52k53D0beCKVTh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}